{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations in pytorch using triplet loss\n",
    "Along the lines of BPR [1]. \n",
    "\n",
    "[1] Rendle, Steffen, et al. \"BPR: Bayesian personalized ranking from implicit feedback.\" Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2009.\n",
    "\n",
    "This is implemented (more efficiently) in LightFM (https://github.com/lyst/lightfm). See the MovieLens example (https://github.com/lyst/lightfm/blob/master/examples/movielens/example.ipynb) for results comparable to this notebook.\n",
    "\n",
    "## Set up the architecture\n",
    "A simple dense layer for both users and items: this is exactly equivalent to latent factor matrix when multiplied by binary user and item indices. There are three inputs: users, positive items, and negative items. In the triplet objective we try to make the positive item rank higher than the negative item for that user.\n",
    "\n",
    "Because we want just one single embedding for the items, we use shared weights for the positive and negative item inputs (a siamese architecture).\n",
    "\n",
    "This is all very simple but could be made arbitrarily complex, with more layers, conv layers and so on. I expect we'll be seeing a lot of papers doing just that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import data\n",
    "import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944 1683\n"
     ]
    }
   ],
   "source": [
    "n_latent = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Read data\n",
    "train, test = data.get_movielens_data()\n",
    "n_users, n_items = train.shape\n",
    "print(n_users, n_items)\n",
    "\n",
    "train = train.tocsr()\n",
    "\n",
    "# uid, pid, nid = data.get_triplets(train)  # user, positive_item, negative_item\n",
    "# test_uid, test_pid, test_nid = data.get_triplets(test)\n",
    "# print(len(uid), len(test_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FactNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_users, n_items,\n",
    "                 n_latent,\n",
    "                ):\n",
    "        super(FactNet, self).__init__()\n",
    "        self.user_embedding_layer = nn.Embedding(n_users, n_latent)\n",
    "        self.item_embedding_layer = nn.Embedding(n_items, n_latent)  # both pos and neg items share these params\n",
    "        init.uniform(self.user_embedding_layer.weight, -0.5, 0.5)\n",
    "        init.uniform(self.item_embedding_layer.weight, -0.5, 0.5)  # default was normal\n",
    "        \n",
    "    def predict_score(self, uid, iid):\n",
    "        # TODO: check to see if this handles multiple users and multiple items correctly\n",
    "        user_embedding = self.user_embedding_layer(uid)\n",
    "        item_embedding = self.item_embedding_layer(iid)\n",
    "        score = (user_embedding * item_embedding).sum(dim=1)\n",
    "        return score\n",
    "    \n",
    "        \n",
    "    def forward(self, uid, pid, nid):\n",
    "        # lulzy forward for loss computation\n",
    "        user_embedding = self.user_embedding_layer(uid)\n",
    "        pos_item_embedding = self.item_embedding_layer(pid)\n",
    "        neg_item_embedding = self.item_embedding_layer(nid)\n",
    "        \n",
    "        # torch.dot doesnt take in axis :sadface:\n",
    "        pos_pred = (user_embedding * pos_item_embedding).sum(dim=1)\n",
    "        neg_pred = (user_embedding * neg_item_embedding).sum(dim=1)\n",
    "        return pos_pred, neg_pred\n",
    "\n",
    "    \n",
    "class TripletBPRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TripletBPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pos_pred, neg_pred):\n",
    "        loss = 1.0 - torch.sigmoid(pos_pred - neg_pred)\n",
    "        return loss.mean()\n",
    "\n",
    "net = FactNet(n_users, n_items, n_latent)\n",
    "criterion = TripletBPRLoss()\n",
    "# optimizer = optim.Adam(net.parameters())\n",
    "# optimizer = optim.Adadelta(net.parameters())\n",
    "optimizer = optim.Adagrad(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batcher(iterable, batch_size):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, batch_size))\n",
    "        if not chunk:\n",
    "            return\n",
    "        yield np.array(chunk, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "def bsearch_membership(csr_mat, row_ind, col_ind):\n",
    "    \"\"\" ehh, this is slower than `in`\n",
    "    csr_mat : interaction matrix\n",
    "    row_ind : user_ind\n",
    "    col_ind : item_ind\n",
    "    \"\"\"\n",
    "    start_idx = csr_mat.indptr[row_ind]\n",
    "    stop_idx = csr_mat.indptr[row_ind+1]\n",
    "    pos_vals = csr_mat.indices[start_idx:stop_idx]\n",
    "    insertion_index = bisect_left(pos_vals, col_ind)\n",
    "    return (insertion_index < (stop_idx-start_idx)) and (col_ind == pos_vals[insertion_index])\n",
    "\n",
    "def get_row_nz(csr_mat, col_ind):\n",
    "    \"\"\"faster than csr_mat.get_row for some reason\"\"\"\n",
    "    start_idx = csr_mat.indptr[col_ind]\n",
    "    stop_idx = csr_mat.indptr[col_ind+1]\n",
    "    return csr_mat.indices[start_idx:stop_idx]\n",
    "    \n",
    "\n",
    "# Negative sample generator\n",
    "\n",
    "def sample_neg(user_ind, interactions,\n",
    "               max_samples=10000,\n",
    "               violating_cond=lambda neg_item_ind:True):\n",
    "    # Given a user index, sample a negative item\n",
    "    user_pos_item_inds = get_row_nz(interactions, user_ind)\n",
    "\n",
    "    n_sampled = 0\n",
    "    while n_sampled < max_samples:\n",
    "        neg_item_ind = np.random.randint(0, n_items)\n",
    "        n_sampled += 1\n",
    "        if neg_item_ind not in user_pos_item_inds:\n",
    "            if violating_cond(neg_item_ind):\n",
    "                break\n",
    "    return neg_item_ind, n_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample from training interactions\n",
    "nnz_users, nnz_items = train.nonzero()\n",
    "# preload at the start of every epoch (next epoch can be done on cpu while gpu trains)\n",
    "nid_preload, n_sampled_preload = zip(*(sample_neg(uid, train) for uid in nnz_users))\n",
    "\n",
    "shuffle_ind = np.arange(len(nnz_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\tAUC: train:0.711888403155\t test:0.614497408772\n",
      "Epoch 1\tAUC: train:0.863969847589\t test:0.79389925349\n",
      "Epoch 2\tAUC: train:0.891934784103\t test:0.838591281236\n",
      "Epoch 3\tAUC: train:0.900436521877\t test:0.85183859534\n",
      "Epoch 4\tAUC: train:0.904506828538\t test:0.857072358836\n",
      "Epoch 5\tAUC: train:0.907181644964\t test:0.86029888901\n",
      "Epoch 6\tAUC: train:0.909430940346\t test:0.863020339457\n",
      "Epoch 7\tAUC: train:0.911610463608\t test:0.865377905644\n",
      "Epoch 8\tAUC: train:0.913632933705\t test:0.867537522349\n",
      "Epoch 9\tAUC: train:0.915571533104\t test:0.869446795057\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch %s' % epoch, end='\\t')\n",
    "    \n",
    "    # Resample negs per pos interaction\n",
    "    nid_preload, n_sampled_preload = map(np.array, zip(*(sample_neg(uid, train) for uid in nnz_users)))\n",
    "\n",
    "    np.random.shuffle(shuffle_ind)\n",
    "\n",
    "    uid_gen = batcher(nnz_users[shuffle_ind], batch_size)\n",
    "    pid_gen = batcher(nnz_items[shuffle_ind], batch_size)\n",
    "    nid_gen = batcher(nid_preload[shuffle_ind], batch_size)\n",
    "    n_samp_gen = batcher(n_sampled_preload[shuffle_ind], batch_size)\n",
    "    \n",
    "    for uid_batch, pid_batch, nid_batch in zip(uid_gen, pid_gen, nid_gen):\n",
    "        user_input = Variable(torch.LongTensor(uid_batch))\n",
    "        p_item_input = Variable(torch.LongTensor(pid_batch))\n",
    "        n_item_input = Variable(torch.LongTensor(nid_batch))\n",
    "        out = net(user_input, p_item_input, n_item_input)\n",
    "        loss = criterion(*out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('AUC: train:{}\\t test:{}'.format(metrics.full_auc_pytorch(net, train),\n",
    "                                           metrics.full_auc_pytorch(net, test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
